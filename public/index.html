<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Sesame Voice Chat</title>
  <style>
    body { font-family: sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
    #log { white-space: pre-wrap; background: #f0f0f0; padding: 10px; height: 200px; overflow-y: auto; font-family: monospace; border-radius: 4px; margin-top: 20px; }
    button { padding: 8px 16px; margin: 5px; background: #4a90e2; color: white; border: none; border-radius: 4px; cursor: pointer; font-size: 16px; }
    button:disabled { background: #bbb; }
    .controls { margin: 20px 0; }
    @media (max-width: 600px) {
      button, #characterSelect, #micSelect { margin: 5px 0; width: 100%; }
    }
  </style>
</head>
<body>
  <h1>Sesame Voice Chat</h1>
  <div id="status" class="disconnected">Disconnected</div>

  <div class="controls">
    <select id="characterSelect">
      <option value="Maya">Maya</option>
      <option value="Miles">Miles</option>
    </select>
    <select id="micSelect"></select>
    <button id="startBtn">Start Session</button>
    <button id="stopBtn" disabled>Stop Session</button>
  </div>

  <div id="log"></div>

  <script>
  'use strict';

  const DEBUG_MODE = true;
  const SAMPLE_RATE = 16000;
  const AUDIO_CODEC = 'none';
  const CLIENT_NAME = 'RP-Web';
  const TIMEZONE = 'America/Chicago';

  let ws, sessionId, callId = null;
  let mediaStream, audioContext;
  let micSource, scriptNode; // For audio input
  let selectedMicId = null;
  let audioQueue = [], isPlaying = false;
  let serverSampleRate = 24000;
  let energyHistory = [];
  let audioLevelInterval;

  const statusEl = document.getElementById('status');
  const logEl    = document.getElementById('log');
  const startBtn = document.getElementById('startBtn');
  const stopBtn  = document.getElementById('stopBtn');
  const charSel  = document.getElementById('characterSelect');
  const micSel   = document.getElementById('micSelect');

  function log(msg, obj = null) {
    if (!DEBUG_MODE && /debug/i.test(msg)) return;
    const t = new Date().toLocaleTimeString();
    logEl.textContent += `[${t}] ${msg}\n`;
    if (obj && DEBUG_MODE) logEl.textContent += `[${t}] ${JSON.stringify(obj, null, 2)}\n`;
    logEl.scrollTop = logEl.scrollHeight;
    if (DEBUG_MODE) console.log(msg, obj);
  }

  function updateStatus(state) {
    statusEl.className = state;
    statusEl.textContent = state.charAt(0).toUpperCase() + state.slice(1);
    log(`Status updated: ${state}`);
  }

  function genRequestId() {
    return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, c => {
      const r = Math.random()*16|0, v = c=='x'? r : (r&0x3|0x8);
      return v.toString(16);
    });
  }

  function base64ToArrayBuffer(b64) {
    const bin = atob(b64);
    const buf = new ArrayBuffer(bin.length);
    const arr = new Uint8Array(buf);
    for (let i = 0; i < bin.length; i++) arr[i] = bin.charCodeAt(i);
    return buf;
  }

  async function playAudio(buffer) {
    if (!audioContext) return;
    try {
      const int16 = new Int16Array(buffer);
      const float32 = new Float32Array(int16.length);
      for (let i = 0; i < int16.length; i++) float32[i] = int16[i] / 32768;
      const audioBuf = audioContext.createBuffer(1, float32.length, serverSampleRate);
      audioBuf.getChannelData(0).set(float32);
      const src = audioContext.createBufferSource();
      src.buffer = audioBuf;
      src.connect(audioContext.destination);
      return new Promise(res => { src.onended = res; src.start(); });
    } catch (e) {
      log('Audio processing error:', e);
      return Promise.resolve();
    }
  }

  function processQueue() {
    if (isPlaying || audioQueue.length === 0) return;
    isPlaying = true;
    playAudio(audioQueue.shift())
      .catch(e => {
        log('Audio playback error:', e);
      })
      .finally(() => {
        isPlaying = false;
        requestAnimationFrame(processQueue);
      });
  }

  // --- Utility for safe callId usage
  function callIdString(ci) {
    if (!ci) return '';
    if (typeof ci === "string") return ci;
    if (typeof ci === "object" && ci.id) return ci.id;
    return String(ci);
  }

  // --- Improved Device Enumeration
  async function populateMicList() {
    micSel.innerHTML = '';
    let devices = [];
    try {
      // Prompt for mic permission so enumerateDevices returns actual labels
      await navigator.mediaDevices.getUserMedia({ audio: true });
      devices = await navigator.mediaDevices.enumerateDevices();
    } catch (e) {
      log('Microphone permission or device error:', e);
    }
    const mics = devices.filter(d => d.kind === 'audioinput');
    if (mics.length === 0) {
      const option = document.createElement('option');
      option.value = '';
      option.text = 'No microphones found';
      micSel.appendChild(option);
      micSel.disabled = true;
    } else {
      mics.forEach((device, i) => {
        const option = document.createElement('option');
        option.value = device.deviceId;
        option.text = device.label || `Microphone ${i+1}`;
        micSel.appendChild(option);
      });
      micSel.disabled = false;
    }
  }

  // --- Microphone streaming logic with audio energy visualization ---
  async function startMic() {
    if (mediaStream) {
      mediaStream.getTracks().forEach(t => t.stop());
      mediaStream = null;
    }
    if (audioContext) {
      try { audioContext.close(); } catch (e) {}
      audioContext = null;
    }
    audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: SAMPLE_RATE });

    // Use the selected mic device
    const constraints = {
      audio: {
        deviceId: selectedMicId ? { exact: selectedMicId } : undefined,
        channelCount: 1,
        sampleRate: SAMPLE_RATE,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true
      }
    };
    try {
      mediaStream = await navigator.mediaDevices.getUserMedia(constraints);
      micSource = audioContext.createMediaStreamSource(mediaStream);
      scriptNode = audioContext.createScriptProcessor(1024, 1, 1); // match Python CHUNK

      micSource.connect(scriptNode);
      // No feedback to speakers; for energy metering only
      const gainNode = audioContext.createGain();
      gainNode.gain.value = 0;
      scriptNode.connect(gainNode);
      gainNode.connect(audioContext.destination);

      scriptNode.onaudioprocess = function(ev) {
        const input = ev.inputBuffer.getChannelData(0);
        // Convert to Int16 PCM
        const pcm = new Int16Array(input.length);
        for (let i = 0; i < input.length; ++i) {
          let s = Math.max(-1, Math.min(1, input[i]));
          pcm[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }
        // Calculate energy for visualization
        const energy = Math.sqrt(pcm.reduce((acc, v) => acc + v*v, 0) / pcm.length);
        energyHistory.push(energy);
        if (energyHistory.length > 5) energyHistory.shift();

        // Visualize energy
        if (!audioLevelInterval) {
          audioLevelInterval = setInterval(() => {
            const avg = energyHistory.length ? energyHistory.reduce((a,b)=>a+b,0)/energyHistory.length : 0;
            const bars = Math.min(20, Math.floor(avg/500));
            log(`Mic: [${'|'.repeat(bars)}${' '.repeat(20-bars)}] Level: ${avg.toFixed(0)}`);
          }, 200);
        }

        // Send to websocket
        if (ws && ws.readyState === WebSocket.OPEN && callId) {
          const b64 = btoa(String.fromCharCode(...new Uint8Array(pcm.buffer)));
          ws.send(JSON.stringify({
            type: 'audio',
            session_id: sessionId,
            call_id: callIdString(callId),
            content: { audio_data: b64 }
          }));
          log('Mic audio sent over WebSocket.', {len: pcm.length, sample: Array.from(pcm.slice(0,8))});
        }
      };

      log('Microphone access granted.');
    } catch (err) {
      log('Error accessing microphone: ' + (err && (err.message || typeof err)));
      updateStatus('disconnected');
      stopSession();
    }
  }

  // --- Device selector logic ---
  micSel.onchange = () => {
    selectedMicId = micSel.value;
    log('Selected mic device: ' + selectedMicId);
  };

  function connect(wsUrl, character) {
    updateStatus('connecting');
    log(`→ Connecting to WebSocket... URL: ${wsUrl}`);
    try {
      ws = new WebSocket(wsUrl);
    } catch (e) {
      log(`× WS Connection Error: ${e.message || e}`);
      updateStatus('disconnected');
      startBtn.disabled = false;
      stopBtn.disabled = true;
      return;
    }

    ws.onopen = () => {
      try {
        log('→ WebSocket connected');
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        audioContext.resume().catch(e => {
          log(`Audio Context resume error: ${e.message}`);
        });
        updateStatus('connected');
      } catch (e) {
        log(`× Audio Context Error: ${e.message || e}`);
        ws.close();
      }
    };

    ws.onmessage = ev => {
      try {
        if (typeof ev.data === 'string') {
          const msg = JSON.parse(ev.data);
          if (msg.type === 'initialize') {
            sessionId = msg.content?.session_id || msg.session_id;
            log(`→ Session initialized: ${sessionId?.substring(0,8)}...`, msg);

            ws.send(JSON.stringify({
              type: 'client_location_state',
              session_id: sessionId,
              call_id: null,
              content: { latitude:0, longitude:0, address:'', timezone:TIMEZONE }
            }));

            ws.send(JSON.stringify({
              type: 'call_connect',
              session_id: sessionId,
              call_id: null,
              request_id: genRequestId(),
              content: {
                sample_rate: SAMPLE_RATE,
                audio_codec: AUDIO_CODEC,
                reconnect: false,
                is_private: false,
                client_name: CLIENT_NAME,
                settings: { preset: character },
                client_metadata: {
                  language: 'en-US',
                  user_agent: navigator.userAgent,
                  mobile_browser: /Mobi|Android/i.test(navigator.userAgent),
                  media_devices: []
                }
              }
            }));
          }
          else if (msg.type === 'call_connect_response' || (msg.type === 'chat' && (msg.call_id || msg.content?.call_id))) {
            callId = msg.call_id || msg.content?.call_id;
            // Fix: always use callIdString to avoid errors
            log(`→ Call connected: ${callIdString(callId).substring(0,8)}...`);
            if (msg.content && msg.content.sample_rate) {
              serverSampleRate = msg.content.sample_rate;
              log(`Server sample rate set: ${serverSampleRate}`);
            }
            startMic();
          }
          else if (msg.type === 'audio') {
            const b64 = msg.content?.audio_data;
            if (b64) {
              const buf = base64ToArrayBuffer(b64);
              audioQueue.push(buf);
              if (!isPlaying) requestAnimationFrame(processQueue);
            }
          }
          else if (msg.type === 'error') {
            log(`× Service error: ${msg.content?.error || 'Unknown error'}`);
          }
        }
      } catch (e) {
        log(`× Message processing error: ${e.message || e}`);
        log(`→ Message data: ${typeof ev.data === "string" ? ev.data.substring(0, 100) : ""}...`);
      }
    };

    ws.onclose = () => {
      updateStatus('disconnected');
      log('× WebSocket closed');
      if (mediaStream) {
        mediaStream.getTracks().forEach(t => t.stop());
        mediaStream = null;
      }
      startBtn.disabled = false;
      stopBtn.disabled = true;
    };

    ws.onerror = e => {
      log(`× WebSocket error: ${e.message || 'Unknown error'}`, e);
    };
  }

  async function startSession() {
    logEl.textContent = '';
    log('Starting session...');
    startBtn.disabled = true;
    stopBtn.disabled = false;
    updateStatus('fetching');

    const char = charSel.value;
    selectedMicId = micSel.value;
    try {
      const res = await fetch(`/capture-websocket/${char.toLowerCase()}`);
      if (!res.ok) {
        const text = await res.text();
        log(`× Server error: ${res.status} ${text}`);
        startBtn.disabled = false;
        stopBtn.disabled = true;
        updateStatus('disconnected');
        return;
      }

      const j = await res.json();
      if (!j.success) {
        log(`× Error: ${j.error}`);
        startBtn.disabled = false;
        stopBtn.disabled = true;
        updateStatus('disconnected');
        return;
      }

      log(`→ Received WebSocket URL for ${j.character}: ${j.websocketUrl}`);
      connect(j.websocketUrl, char);
    } catch (e) {
      log(`× Fetch error: ${e.message || e}`);
      startBtn.disabled = false;
      stopBtn.disabled = true;
      updateStatus('disconnected');
    }
  }

  function stopSession() {
    log('Stopping session...');
    audioQueue = [];
    isPlaying = false;

    if (mediaStream) {
      mediaStream.getTracks().forEach(t => {
        try { t.stop(); t.enabled = false; } catch (e) { log('Track stop error:', e); }
      });
      mediaStream = null;
      log('MediaStream tracks stopped.');
    }

    if (audioContext) {
      try { audioContext.close(); } catch (e) { log('Audio context close error:', e); }
      audioContext = null;
      log('AudioContext closed.');
    }

    if (audioLevelInterval) {
      clearInterval(audioLevelInterval);
      audioLevelInterval = null;
    }

    let closeWs = true;

    if (ws && ws.readyState === WebSocket.OPEN && callId && sessionId) {
      try {
        ws.send(JSON.stringify({
          type: 'call_disconnect',
          session_id: sessionId,
          call_id: callIdString(callId),
          request_id: genRequestId(),
          content: { reason: 'user_request' }
        }));
        closeWs = false;
        setTimeout(() => {
          if (ws) {
            try { ws.close(); } catch (e) {}
            ws = null;
          }
          sessionId = null;
          callId = null;
          log('WebSocket closed after disconnect.');
        }, 100);
      } catch (err) {
        log('Error sending disconnect:', err);
      }
    }

    if (closeWs && ws) {
      try { ws.close(); } catch (e) {}
      ws = null;
      sessionId = null;
      callId = null;
      log('WebSocket closed immediately.');
    }

    startBtn.disabled = false;
    stopBtn.disabled = true;
    updateStatus('disconnected');
  }

  window.addEventListener('load', () => {
    if (!window.WebSocket) {
      log('Error: WebSockets not supported in this browser');
      startBtn.disabled = true;
    }
    if (!window.AudioContext && !window.webkitAudioContext) {
      log('Error: Web Audio API not supported in this browser');
      startBtn.disabled = true;
    }
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      log('Error: getUserMedia not supported in this browser');
      startBtn.disabled = true;
    }
    populateMicList();
  });

  startBtn.onclick = startSession;
  stopBtn.onclick = stopSession;
  micSel.onfocus = populateMicList; // Update list if user clicks before session
  updateStatus('disconnected');
  </script>
</body>
</html>
