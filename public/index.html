<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Sesame Voice Chat</title>
  <style>
    body { font-family: sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
    #log { white-space: pre-wrap; background: #f0f0f0; padding: 10px; height: 200px; overflow-y: auto; font-family: monospace; border-radius: 4px; margin-top: 20px; }
    button { padding: 8px 16px; margin: 5px; background: #4a90e2; color: white; border: none; border-radius: 4px; cursor: pointer; font-size: 16px; }
    button:disabled { background: #bbb; }
    .controls { margin: 20px 0; }
    @media (max-width: 600px) {
      button, #characterSelect, #micSelect { margin: 5px 0; width: 100%; }
    }
  </style>
</head>
<body>
  <h1>Sesame Voice Chat</h1>
  <div id="status" class="disconnected">Disconnected</div>

  <div class="controls">
    <select id="characterSelect">
      <option value="Maya">Maya</option>
      <option value="Miles">Miles</option>
    </select>
    <select id="micSelect"></select>
    <button id="startBtn">Start Session</button>
    <button id="stopBtn" disabled>Stop Session</button>
  </div>

  <div id="log"></div>

  <script>
  'use strict';

  const DEBUG_MODE = true;
  const SAMPLE_RATE = 16000;
  const AUDIO_CODEC = 'none';
  const CLIENT_NAME = 'RP-Web';
  const TIMEZONE = 'America/Chicago';

  let ws, sessionId, callId = null;
  let mediaStream, audioContext;
  let audioQueue = [], isPlaying = false;
  let serverSampleRate = 24000;

  const statusEl = document.getElementById('status');
  const logEl    = document.getElementById('log');
  const startBtn = document.getElementById('startBtn');
  const stopBtn  = document.getElementById('stopBtn');
  const charSel  = document.getElementById('characterSelect');
  const micSel   = document.getElementById('micSelect');

  function log(msg, obj = null) {
    if (!DEBUG_MODE && /debug/i.test(msg)) return;
    const t = new Date().toLocaleTimeString();
    logEl.textContent += `[${t}] ${msg}\n`;
    if (obj && DEBUG_MODE) logEl.textContent += `[${t}] ${JSON.stringify(obj, null, 2)}\n`;
    logEl.scrollTop = logEl.scrollHeight;
    if (DEBUG_MODE) console.log(msg, obj);
  }

  function updateStatus(state) {
    statusEl.className = state;
    statusEl.textContent = state.charAt(0).toUpperCase() + state.slice(1);
    log(`Status updated: ${state}`);
  }

  function genRequestId() {
    return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, c => {
      const r = Math.random()*16|0, v = c=='x'? r : (r&0x3|0x8);
      return v.toString(16);
    });
  }

  function base64ToArrayBuffer(b64) {
    const bin = atob(b64);
    const buf = new ArrayBuffer(bin.length);
    const arr = new Uint8Array(buf);
    for (let i = 0; i < bin.length; i++) arr[i] = bin.charCodeAt(i);
    return buf;
  }

  async function playAudio(buffer) {
    if (!audioContext) return;
    try {
      const int16 = new Int16Array(buffer);
      const float32 = new Float32Array(int16.length);
      for (let i = 0; i < int16.length; i++) float32[i] = int16[i] / 32768;
      const audioBuf = audioContext.createBuffer(1, float32.length, serverSampleRate);
      audioBuf.getChannelData(0).set(float32);
      const src = audioContext.createBufferSource();
      src.buffer = audioBuf;
      src.connect(audioContext.destination);
      return new Promise(res => { src.onended = res; src.start(); });
    } catch (e) {
      log('Audio processing error:', e);
      return Promise.resolve();
    }
  }

  function processQueue() {
    if (isPlaying || audioQueue.length === 0) return;
    isPlaying = true;
    playAudio(audioQueue.shift())
      .catch(e => {
        log('Audio playback error:', e);
      })
      .finally(() => {
        isPlaying = false;
        requestAnimationFrame(processQueue);
      });
  }

  function populateMicList() {
    micSel.innerHTML = '';
    navigator.mediaDevices.enumerateDevices()
      .then(devices => {
        const mics = devices.filter(d => d.kind === 'audioinput');
        if (mics.length === 0) {
          const option = document.createElement('option');
          option.value = '';
          option.text = 'No microphones found';
          micSel.appendChild(option);
          micSel.disabled = true;
        } else {
          mics.forEach((device, i) => {
            const option = document.createElement('option');
            option.value = device.deviceId;
            option.text = device.label || `Microphone ${i+1}`;
            micSel.appendChild(option);
          });
          micSel.disabled = false;
        }
      })
      .catch(e => {
        log('Error enumerating devices:', e);
        micSel.disabled = true;
      });
  }

  function startMic() {
    log('Attempting to start microphone...');
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      log('Error: getUserMedia not supported in this browser');
      updateStatus('disconnected');
      stopSession();
      return;
    }

    const selectedMic = micSel.value;
    navigator.mediaDevices.getUserMedia({
      audio: {
        deviceId: selectedMic ? { exact: selectedMic } : undefined,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
        sampleRate: SAMPLE_RATE
      }
    })
    .then(stream => {
      log('Microphone access granted.');
      mediaStream = stream;
      const src = audioContext.createMediaStreamSource(stream);
      const proc = audioContext.createScriptProcessor(2048, 1, 1);

      src.connect(proc);

      const gainNode = audioContext.createGain();
      gainNode.gain.value = 0;
      proc.connect(gainNode);
      gainNode.connect(audioContext.destination);

      proc.onaudioprocess = ev => {
        if (!ws || ws.readyState !== WebSocket.OPEN || !callId) {
          log('Audio processing skipped: WebSocket not open or callId missing.');
          return;
        }
        const inF32 = ev.inputBuffer.getChannelData(0);
        const pcm = new Int16Array(inF32.length);
        for (let i = 0; i < inF32.length; i++) {
          const s = Math.max(-1, Math.min(1, inF32[i]));
          pcm[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }
        const b64 = btoa(String.fromCharCode(...new Uint8Array(pcm.buffer)));
        try {
          const actualCallId = typeof callId === 'object' && callId !== null && callId.id ? callId.id : callId;
          ws.send(JSON.stringify({
            type: 'audio',
            session_id: sessionId,
            call_id: actualCallId,
            content: { audio_data: b64 }
          }));
          log('Mic audio sent over WebSocket.', {len: pcm.length, sample: pcm.slice(0,8)});
        } catch (e) {
          log('WebSocket send error:', e);
        }
      };
    })
    .catch(e => {
      log(`Error accessing microphone: ${e.message || e}`);
      updateStatus('disconnected');
      stopSession();
    });
  }

  function connect(wsUrl, character) {
    updateStatus('connecting');
    log(`→ Connecting to WebSocket... URL: ${wsUrl}`);
    try {
      ws = new WebSocket(wsUrl);
    } catch (e) {
      log(`× WS Connection Error: ${e.message || e}`);
      updateStatus('disconnected');
      startBtn.disabled = false;
      stopBtn.disabled = true;
      return;
    }

    ws.onopen = () => {
      try {
        log('→ WebSocket connected');
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        audioContext.resume().catch(e => {
          log(`Audio Context resume error: ${e.message}`);
        });
        updateStatus('connected');
      } catch (e) {
        log(`× Audio Context Error: ${e.message || e}`);
        ws.close();
      }
    };

    ws.onmessage = ev => {
      try {
        if (typeof ev.data === 'string') {
          const msg = JSON.parse(ev.data);
          if (msg.type === 'initialize') {
            sessionId = msg.content?.session_id || msg.session_id;
            log(`→ Session initialized: ${sessionId?.substring(0,8)}...`, msg);

            ws.send(JSON.stringify({
              type: 'client_location_state',
              session_id: sessionId,
              call_id: null,
              content: { latitude:0, longitude:0, address:'', timezone:TIMEZONE }
            }));

            ws.send(JSON.stringify({
              type: 'call_connect',
              session_id: sessionId,
              call_id: null,
              request_id: genRequestId(),
              content: {
                sample_rate: SAMPLE_RATE,
                audio_codec: AUDIO_CODEC,
                reconnect: false,
                is_private: false,
                client_name: CLIENT_NAME,
                settings: { preset: character },
                client_metadata: {
                  language: 'en-US',
                  user_agent: navigator.userAgent,
                  mobile_browser: /Mobi|Android/i.test(navigator.userAgent),
                  media_devices: []
                }
              }
            }));
          }
          else if (msg.type === 'call_connect_response' || (msg.type === 'chat' && (msg.call_id || msg.content?.call_id))) {
            callId = msg.call_id || msg.content?.call_id;
            if (msg.content && msg.content.sample_rate) {
              serverSampleRate = msg.content.sample_rate;
              log(`Server sample rate set: ${serverSampleRate}`);
            }
            log(`→ Call connected: ${callId?.substring(0,8)}...`);
            startMic();
          }
          else if (msg.type === 'audio') {
            const b64 = msg.content?.audio_data;
            if (b64) {
              const buf = base64ToArrayBuffer(b64);
              audioQueue.push(buf);
              if (!isPlaying) requestAnimationFrame(processQueue);
            }
          }
          else if (msg.type === 'error') {
            log(`× Service error: ${msg.content?.error || 'Unknown error'}`);
          }
        }
      } catch (e) {
        log(`× Message processing error: ${e.message || e}`);
        log(`→ Message data: ${ev.data.substring(0, 100)}...`);
      }
    };

    ws.onclose = () => {
      updateStatus('disconnected');
      log('× WebSocket closed');
      if (mediaStream) {
        mediaStream.getTracks().forEach(t => t.stop());
        mediaStream = null;
      }
      startBtn.disabled = false;
      stopBtn.disabled = true;
    };

    ws.onerror = e => {
      log(`× WebSocket error: ${e.message || 'Unknown error'}`, e);
    };
  }

  async function startSession() {
    logEl.textContent = '';
    log('Starting session...');
    startBtn.disabled = true;
    stopBtn.disabled = false;
    updateStatus('fetching');

    const char = charSel.value;
    try {
      const res = await fetch(`/capture-websocket/${char.toLowerCase()}`);
      if (!res.ok) {
        const text = await res.text();
        log(`× Server error: ${res.status} ${text}`);
        startBtn.disabled = false;
        stopBtn.disabled = true;
        updateStatus('disconnected');
        return;
      }

      const j = await res.json();
      if (!j.success) {
        log(`× Error: ${j.error}`);
        startBtn.disabled = false;
        stopBtn.disabled = true;
        updateStatus('disconnected');
        return;
      }

      log(`→ Received WebSocket URL for ${j.character}: ${j.websocketUrl}`);
      connect(j.websocketUrl, char);
    } catch (e) {
      log(`× Fetch error: ${e.message || e}`);
      startBtn.disabled = false;
      stopBtn.disabled = true;
      updateStatus('disconnected');
    }
  }

  function stopSession() {
    log('Stopping session...');
    audioQueue = [];
    isPlaying = false;

    if (mediaStream) {
      mediaStream.getTracks().forEach(t => {
        try { t.stop(); t.enabled = false; } catch (e) { log('Track stop error:', e); }
      });
      mediaStream = null;
      log('MediaStream tracks stopped.');
    }

    if (audioContext) {
      try { audioContext.close(); } catch (e) { log('Audio context close error:', e); }
      audioContext = null;
      log('AudioContext closed.');
    }

    let closeWs = true;

    if (ws && ws.readyState === WebSocket.OPEN && callId && sessionId) {
      try {
        const actualCallId = typeof callId === 'object' && callId !== null && callId.id ? callId.id : callId;
        ws.send(JSON.stringify({
          type: 'call_disconnect',
          session_id: sessionId,
          call_id: actualCallId,
          request_id: genRequestId(),
          content: { reason: 'user_request' }
        }));
        closeWs = false;
        setTimeout(() => {
          if (ws) {
            try { ws.close(); } catch (e) {}
            ws = null;
          }
          sessionId = null;
          callId = null;
          log('WebSocket closed after disconnect.');
        }, 100);
      } catch (err) {
        log('Error sending disconnect:', err);
      }
    }

    if (closeWs && ws) {
      try { ws.close(); } catch (e) {}
      ws = null;
      sessionId = null;
      callId = null;
      log('WebSocket closed immediately.');
    }

    startBtn.disabled = false;
    stopBtn.disabled = true;
    updateStatus('disconnected');
  }

  window.addEventListener('load', () => {
    if (!window.WebSocket) {
      log('Error: WebSockets not supported in this browser');
      startBtn.disabled = true;
    }
    if (!window.AudioContext && !window.webkitAudioContext) {
      log('Error: Web Audio API not supported in this browser');
      startBtn.disabled = true;
    }
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      log('Error: getUserMedia not supported in this browser');
      startBtn.disabled = true;
    }
    populateMicList();
  });

  startBtn.onclick = startSession;
  stopBtn.onclick = stopSession;
  micSel.onfocus = populateMicList; // Update list if user clicks before session
  updateStatus('disconnected');
  </script>
</body>
</html>
